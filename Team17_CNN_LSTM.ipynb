{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Team17_CNN_LSTM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVM6C8bs/f55E6Q11GLwkl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Whitchurch/Tensorflow/blob/main/Team17_CNN_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJCjQ8fSb-M1"
      },
      "source": [
        "#!pip install tensorflow-GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI8e-OL5rtyG",
        "outputId": "0d3b5374-448c-4fef-c53e-db57b9dbac6a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxKbf6z3cT3G"
      },
      "source": [
        "Install Tensorflow on Google Co-lab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zNXb0FNcr7S"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import csv \n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import AveragePooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import CuDNNLSTM # A superior LSTM that uses GPU more optimized for training.\n",
        "from keras.layers import LSTM\n",
        "from keras.models import Sequential"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YCMbF1LslRM"
      },
      "source": [
        "Steps to pre-process and load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q76uho-Lsp47"
      },
      "source": [
        "os.chdir(\"drive/My Drive/Colab Notebooks\")\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPKqnFBNszh3",
        "outputId": "9b456099-27d8-4d16-b2d5-1e108a13a358"
      },
      "source": [
        "ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Access_Local_directory.ipynb\n",
            "'Copy of Untitled0.ipynb'\n",
            " ECGsignal.csv\n",
            " ECGsignal_n.csv\n",
            " ECGsignal_nwhite.csv\n",
            " Exercise_1_Cats_vs_Dogs_Question-FINAL.ipynb\n",
            " Exercise_2_Cats_vs_Dogs_using_augmentation_Question-FINAL.ipynb\n",
            " Exercise_3_Horses_vs_humans_using_Transfer_Learning_Question-FINAL.ipynb\n",
            " Exercise_4_Multi_class_classifier_Question-FINAL.ipynb\n",
            " FinalCNN_Beginner.ipynb\n",
            " Team17_CNN_LSTM.ipynb\n",
            " Untitled0.ipynb\n",
            " Untitled1.ipynb\n",
            " Untitled2.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_WYmzcttPiG",
        "outputId": "ff6d5d12-54ca-4131-de91-a5dfe397ab4c"
      },
      "source": [
        "#Read the clean signal\n",
        "with open('ECGsignal.csv') as csv_file:\n",
        "  csv_reader = csv.reader(csv_file)\n",
        "\n",
        "  for ECGsignal in csv_reader:\n",
        "    print(ECGsignal)\n",
        "  print(\"Finished reading the file\")\n",
        "\n",
        "#Time to dissect the file read data's anatomy:\n",
        "#Total number of items in the list:\n",
        "len(ECGsignal)\n",
        "\n",
        "#Convert the list to an array \n",
        "ECGsignal_arr = np.array(ECGsignal,dtype=float)\n",
        "\n",
        "#Read the noisy signal\n",
        "with open('ECGsignal_n.csv') as csv_file:\n",
        "  csv_reader = csv.reader(csv_file)\n",
        "\n",
        "  for ECGsignal_n in csv_reader:\n",
        "    print(ECGsignal_n)\n",
        "  print(\"Finished reading the file\")\n",
        "\n",
        "#Time to dissect the file read data's anatomy:\n",
        "#Total number of items in the list:\n",
        "len(ECGsignal_n)\n",
        "\n",
        "#Convert the list to an array \n",
        "ECGsignal_n_arr = np.array(ECGsignal_n,dtype=float)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2YBfWXfv5Ke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e519028-0dc3-4b22-efea-27273efa189f"
      },
      "source": [
        "tupleofdimensions = ECGsignal_arr.shape\n",
        "num = tupleofdimensions[0]\n",
        "Training_test_Split = 0.80  # Modify this line to control the Test-Train split\n",
        "\n",
        "a=Training_test_Split*num;                                 \n",
        "XTrain=ECGsignal_n_arr[0:int(a)];                  \n",
        "YTrain=ECGsignal_arr[0:int(a)];\n",
        "XTest=ECGsignal_n_arr[int(a):num];               \n",
        "YTest=ECGsignal_arr[int(a):num];\n",
        "\n",
        "train_samples=30*60*Training_test_Split/10;\n",
        "test_samples =30*60*(1-Training_test_Split)/10;\n",
        "\n",
        "print(int(train_samples))\n",
        "print(int(test_samples))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "144\n",
            "35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94xeGV8y1Gyg"
      },
      "source": [
        "Break up the Train and Test data into 4D vectors for the Tensorflow CNN: Format is [Batch_size,Height,width,depth]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQwVYV0r1UYy"
      },
      "source": [
        "XTrain_reshaped = np.reshape(XTrain,(int(train_samples),30000,1,1))\n",
        "YTrain_reshaped = np.reshape(YTrain,(int(train_samples),30000))\n",
        "\n",
        "XTest_reshaped = np.reshape(XTest,(int(test_samples),30000,1,1))\n",
        "YTest_reshaped = np.reshape(YTest,(int(test_samples),30000))\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DS4G6DNdkC3"
      },
      "source": [
        "Start defining the layers of the CNN - From the De-noising paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppaaL62fdjsJ"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Convolution layers are used to extract the most prominent features of the input data.\n",
        "#Layer 1:\n",
        "#Conv-layer -> BatchNorm -> RELU-> AvdPooling\n",
        "model.add(Conv2D(36, (19,1), input_shape=(30000,1,1),activation='relu',padding='SAME',strides=(1,1),name='conv_1')) \n",
        "model.add(BatchNormalization(name='batchnorm_1'))\n",
        "model.add(Dense(units=36,activation='relu',name='relu_1'))\n",
        "model.add(AveragePooling2D(pool_size=(2,1),strides=(4,1),name='avgpool_1'))\n",
        "\n",
        "#Layer 2:\n",
        "#Conv-layer -> BatchNorm -> RELU-> AvdPooling\n",
        "model.add(Conv2D(36,(19,1),strides=(1,1),padding='SAME',name='conv_2'))\n",
        "model.add(BatchNormalization(name='batchnorm_2'))\n",
        "model.add(Dense(units=36,activation='relu',name='relu_2'))\n",
        "model.add(AveragePooling2D(pool_size=(2,1),strides=(4,1),name='avgpool_2'))\n",
        "\n",
        "#Layer 3:\n",
        "#Conv-layer -> BatchNorm -> RELU-> AvdPooling\n",
        "model.add(Conv2D(36,(19,1),strides=(1,1),padding='SAME',name='conv_3'))\n",
        "model.add(BatchNormalization(name='batchnorm_3'))\n",
        "model.add(Dense(units=36,activation='relu',name='relu_3'))\n",
        "model.add(AveragePooling2D(pool_size=(2,1),strides=(4,1),name='avgpool_3'))\n",
        "\n",
        "#Layer 4:\n",
        "#Conv-layer -> BatchNorm -> RELU-> AvdPooling\n",
        "model.add(Conv2D(36,(19,1),strides=(1,1),padding='SAME',name='conv_4'))\n",
        "model.add(BatchNormalization(name='batchnorm_4'))\n",
        "model.add(Dense(units=36,activation='relu',name='relu_4'))\n",
        "model.add(AveragePooling2D(pool_size=(2,1),strides=(4,1),name='avgpool_4'))\n",
        "\n",
        "\n",
        "#Layer 5:\n",
        "#Conv-layer -> BatchNorm -> RELU-> AvdPooling\n",
        "model.add(Conv2D(36,(19,1),strides=(1,1),padding='SAME',name='conv_5'))\n",
        "model.add(BatchNormalization(name='batchnorm_5'))\n",
        "model.add(Dense(units=36,activation='relu',name='relu_5'))\n",
        "model.add(AveragePooling2D(pool_size=(2,1),strides=(4,1),name='avgpool_5'))\n",
        "\n",
        "\n",
        "#Layer 6:\n",
        "#Conv-layer -> BatchNorm -> RELU-> AvdPooling\n",
        "model.add(Conv2D(36,(19,1),strides=(1,1),padding='SAME',name='conv_6'))\n",
        "model.add(BatchNormalization(name='batchnorm_6'))\n",
        "model.add(Dense(units=36,activation='relu',name='relu_6'))\n",
        "model.add(AveragePooling2D(pool_size=(2,1),strides=(4,1),name='avgpool_6'))\n",
        "model.add(Flatten())\n",
        "\n",
        "#Now adding in the fully-connected dense layer: This will act on the data extracted from the CNN in the prior stages\n",
        "model.add(Dense(units= 30000,activation=None,use_bias=True))\n",
        "model.summary()\n",
        "\n",
        "#Last layer of the FCN is only showing the inputsx weight.  \n",
        "#Investigating to see, if the bias is used. even though model summary is not displaying it.\n",
        "#weights, biases = model.layers[25].get_weights()\n",
        "#print(len(weights))\n",
        "#print(len(biases))\n",
        "\n",
        "#Build the model, we follow the paper, using Adam optimizer, for speeding up gradient descent\n",
        "#And RMS as the cost function, metric to optimize against.\n",
        "\n",
        "#Compile the model\n",
        "model.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])\n",
        "\n",
        "#Fit the model.\n",
        "# Train the Model\n",
        "#history = model.fit_generator(train_generator,validation_data = validation_generator, epochs = 10,steps_per_epoch = 1372, validation_steps = 350,verbose = 1 )\n",
        "history = model.fit(XTrain_reshaped,YTrain_reshaped,epochs=15,verbose=1,validation_data=(XTest_reshaped,YTest_reshaped))\n",
        "\n",
        "# Plot the chart for accuracy and loss on both training and validation\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "                                    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOt07WKtDirw"
      },
      "source": [
        "Start defining the LSTM implementation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VaLOkLGxoJ1"
      },
      "source": [
        "RE-shape the Data so that it can be used in the LSTM:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2rC2nZWxr0x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVs4JvKHxkMo"
      },
      "source": [
        "The definition of the LSTM model follows below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe2pXI_YDnc0"
      },
      "source": [
        "model1 = Sequential(name=\"DNN using LSTM for Denoising\");\n",
        "#The LSTM->RELU-> return sequential ouput.\n",
        "model1.add(LSTM(140,input_shape=(30000,1,),name=\"lstm_1_relu_1\",activation='relu',return_sequences=True)) \n",
        "\n",
        "# sequential output/input -> LSTM2->Relu -> flattened output (return_sequence = false, by default)\n",
        "#The output of LSTM2 is a flattened output -> Fully connected layer.\n",
        "model1.add(LSTM(140,name=\"lstm_2_relu_2\",activation='relu'))\n",
        "\n",
        "#Add the fully connected layer of 30000 \n",
        "model1.add(Dense(30000,activation=None))\n",
        "model1.summary()\n",
        "\n",
        "#Build the model, we follow the paper, using Adam optimizer, for speeding up gradient descent\n",
        "#And RMS as the cost function, metric to optimize against.\n",
        "\n",
        "#Compile the model\n",
        "model1.compile(optimizer='adam',loss='mean_squared_error')\n",
        "\n",
        "#Fit the model.\n",
        "# Train the Model\n",
        "history = model.fit_generator(train_generator,validation_data = validation_generator, epochs = 10,steps_per_epoch = 1372, validation_steps = 350,verbose = 1 )\n",
        "\n",
        "# Plot the chart for accuracy and loss on both training and validation\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmjWCEyTgRgC"
      },
      "source": [
        "Helpful references:\n",
        "\n",
        "1)This explains why last dense layer has no activation function:\n",
        "  - we primarily do that if we are trying to accuractely do regression.\n",
        "\n",
        "https://stats.stackexchange.com/questions/361066/what-is-the-point-of-having-a-dense-layer-in-a-neural-network-with-no-activation\n",
        "\n",
        "2)This video shows how to implement a stacked LSTM model:\n",
        "https://www.youtube.com/watch?v=BSpXCRTOLJA\n",
        "\n",
        "3) This video shows the inner anatomy of an LSTM, primarily the sigmoid and Tanh, used for gating inside the LSTM:\n",
        "https://www.youtube.com/watch?v=8HyCNIVRbSU&t=632s\n",
        "\n",
        "4) The blogpost that references the video in 3. Is linked here:\n",
        "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
        "\n",
        "5) This is the link from Andrew NG, I studied to understand, why we use BatchNormalization in the CNN:\n",
        "https://www.youtube.com/watch?v=nUUqwaxLnWs\n",
        "\n",
        "\n"
      ]
    }
  ]
}