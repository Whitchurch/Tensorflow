{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNsQ0XxN7gGygQHJRo7cANn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Whitchurch/Tensorflow/blob/main/Team17_CNN_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJCjQ8fSb-M1"
      },
      "source": [
        "#!pip install tensorflow-GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxKbf6z3cT3G"
      },
      "source": [
        "Install Tensorflow on Google Co-lab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zNXb0FNcr7S"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import AveragePooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import CuDNNLSTM # A superior LSTM that uses GPU more optimized for training.\n",
        "from keras.layers import LSTM\n",
        "from keras.models import Sequential"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DS4G6DNdkC3"
      },
      "source": [
        "Start defining the layers of the CNN - From the De-noising paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppaaL62fdjsJ"
      },
      "source": [
        "model = Sequential(name=\"DNN using CONV for Denoising\")\n",
        "\n",
        "# Convolution layers are used to extract the most prominent features of the input data.\n",
        "#Layer 1:\n",
        "#Conv-layer -> BatchNorm -> RELU-> AvdPooling\n",
        "model.add(Conv2D(36, (19,1), input_shape=(30000,1,1),activation='relu',padding='SAME',strides=(1,1),name='conv_1')) \n",
        "model.add(BatchNormalization(name='batchnorm_1'))\n",
        "model.add(Dense(units=36,activation='relu',name='relu_1'))\n",
        "model.add(AveragePooling2D(pool_size=(2,1),strides=(4,1),name='avgpool_1'))\n",
        "\n",
        "#Layer 2:\n",
        "#Conv-layer -> BatchNorm -> RELU-> AvdPooling\n",
        "model.add(Conv2D(36,(19,1),strides=(1,1),padding='SAME',name='conv_2'))\n",
        "model.add(BatchNormalization(name='batchnorm_2'))\n",
        "model.add(Dense(units=36,activation='relu',name='relu_2'))\n",
        "model.add(AveragePooling2D(pool_size=(2,1),strides=(4,1),name='avgpool_2'))\n",
        "\n",
        "#Layer 3:\n",
        "#Conv-layer -> BatchNorm -> RELU-> AvdPooling\n",
        "model.add(Conv2D(36,(19,1),strides=(1,1),padding='SAME',name='conv_3'))\n",
        "model.add(BatchNormalization(name='batchnorm_3'))\n",
        "model.add(Dense(units=36,activation='relu',name='relu_3'))\n",
        "model.add(AveragePooling2D(pool_size=(2,1),strides=(4,1),name='avgpool_3'))\n",
        "\n",
        "#Layer 4:\n",
        "#Conv-layer -> BatchNorm -> RELU-> AvdPooling\n",
        "model.add(Conv2D(36,(19,1),strides=(1,1),padding='SAME',name='conv_4'))\n",
        "model.add(BatchNormalization(name='batchnorm_4'))\n",
        "model.add(Dense(units=36,activation='relu',name='relu_4'))\n",
        "model.add(AveragePooling2D(pool_size=(2,1),strides=(4,1),name='avgpool_4'))\n",
        "\n",
        "\n",
        "#Layer 5:\n",
        "#Conv-layer -> BatchNorm -> RELU-> AvdPooling\n",
        "model.add(Conv2D(36,(19,1),strides=(1,1),padding='SAME',name='conv_5'))\n",
        "model.add(BatchNormalization(name='batchnorm_5'))\n",
        "model.add(Dense(units=36,activation='relu',name='relu_5'))\n",
        "model.add(AveragePooling2D(pool_size=(2,1),strides=(4,1),name='avgpool_5'))\n",
        "\n",
        "\n",
        "#Layer 6:\n",
        "#Conv-layer -> BatchNorm -> RELU-> AvdPooling\n",
        "model.add(Conv2D(36,(19,1),strides=(1,1),padding='SAME',name='conv_6'))\n",
        "model.add(BatchNormalization(name='batchnorm_6'))\n",
        "model.add(Dense(units=36,activation='relu',name='relu_6'))\n",
        "model.add(AveragePooling2D(pool_size=(2,1),strides=(4,1),name='avgpool_6'))\n",
        "model.add(Flatten())\n",
        "\n",
        "#Now adding in the fully-connected dense layer: This will act on the data extracted from the CNN in the prior stages\n",
        "model.add(Dense(units= 30000,activation=None))\n",
        "\n",
        "model.summary()\n",
        "                                    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOt07WKtDirw"
      },
      "source": [
        "Start defining the LSTM implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pe2pXI_YDnc0",
        "outputId": "457c55b4-cc4a-402d-90b1-239efb9f8026"
      },
      "source": [
        "model1 = Sequential(name=\"DNN using LSTM for Denoising\");\n",
        "#The LSTM->RELU-> return sequential ouput.\n",
        "model1.add(LSTM(140,input_shape=(30000,1,),name=\"lstm_1\",activation='relu',return_sequences=True)) \n",
        "\n",
        "# sequential output/input -> LSTM2->Relu -> flattened output (return_sequence = false, by default)\n",
        "#The output of LSTM2 is a flattened output -> Fully connected layer.\n",
        "model1.add(LSTM(140,name=\"lstm_2\",activation='relu'))\n",
        "\n",
        "#Add the fully connected layer of 30000 \n",
        "model.add(Dense(30000,activation=None))\n",
        "model1.summary()\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"DNN using LSTM for Denoising\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 30000, 140)        79520     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 140)               157360    \n",
            "=================================================================\n",
            "Total params: 236,880\n",
            "Trainable params: 236,880\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmjWCEyTgRgC"
      },
      "source": [
        "Helpful references:\n",
        "\n",
        "1)This explains why last dense layer has no activation function:\n",
        "  - we primarily do that if we are trying to accuractely do regression.\n",
        "\n",
        "https://stats.stackexchange.com/questions/361066/what-is-the-point-of-having-a-dense-layer-in-a-neural-network-with-no-activation\n",
        "\n",
        "2)This video shows how to implement a stacked LSTM model:\n",
        "https://www.youtube.com/watch?v=BSpXCRTOLJA\n",
        "\n",
        "3) This video shows the inner anatomy of an LSTM, primarily the sigmoid and Tanh, used for gating inside the LSTM:\n",
        "https://www.youtube.com/watch?v=8HyCNIVRbSU&t=632s\n",
        "\n",
        "4) The blogpost that references the video in 3. Is linked here:\n",
        "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
        "\n"
      ]
    }
  ]
}